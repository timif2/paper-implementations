{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) implementation\n",
    "\n",
    "This project is a PyTorch implementation of the paper: An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale. It uses the labml module, and is based off of the labmlai implementation. This model can be trained on datasets, including the CIFAR-10, for instance.\n",
    "\n",
    "Vision transformers utilise a transformer architecture exclusively for image processing, omitting convolutional layers. They segment the image into patches and employ a transformer model on these patch embeddings. These embeddings are created by linearly transforming the flattened pixel values of each patch. Subsequently, a standard transformer encoder processes these patch embeddings alongside a classification token. The resultant encoding of the token is utilised for image classification via a Multi-Layer Perceptron (MLP).\n",
    "\n",
    "To address the lack of inherent spatial information in the patch embeddings due to the transformer architecture, learned positional embeddings are incorporated. These positional embeddings, which consist of vectors corresponding to each patch location, are trained alongside other parameters using gradient descent.\n",
    "\n",
    "Vision transformers demonstrate strong performance when pre-trained on extensive datasets. The recommended approach involves pre-training with an MLP classification head, followed by fine-tuning using a single linear layer. Notably, the paper surpasses the state-of-the-art (SOTA) performance with a vision transformer pre-trained on a dataset comprising 300 million images. Additionally, higher-resolution images are employed during inference while maintaining the patch size. The positional embeddings for new patch locations are determined through interpolation of learned positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import TransformerLayer\n",
    "from labml_nn.utils import clone_module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(Module):\n",
    "   \n",
    "    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n",
    "     \n",
    "        super().__init__()\n",
    "\n",
    "        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n",
    "        # This is equivalent to splitting the image into patches and doing a linear\n",
    "        # transformation on each patch.\n",
    "        self.conv = nn.Conv2d(in_channels, d_model, patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      \n",
    "        # Apply convolution layer\n",
    "        x = self.conv(x)\n",
    "        # Get the shape.\n",
    "        bs, c, h, w = x.shape\n",
    "        # Rearrange to shape `[patches, batch_size, d_model]`\n",
    "        x = x.permute(2, 3, 0, 1)\n",
    "        x = x.view(h * w, bs, c)\n",
    "\n",
    "        # Return the patch embeddings\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbeddings(Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5_000):\n",
    "        super().__init__()\n",
    "        # Positional embeddings for each location\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Get the positional embeddings for the given patches\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        # Add to patch embeddings and return\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_hidden: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        # First layer\n",
    "        self.linear1 = nn.Linear(d_model, n_hidden)\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "        # Second layer\n",
    "        self.linear2 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First layer and activation\n",
    "        x = self.act(self.linear1(x))\n",
    "        # Second layer\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        #\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(Module):\n",
    "\n",
    "    def __init__(self, transformer_layer: TransformerLayer, n_layers: int,\n",
    "                 patch_emb: PatchEmbeddings, pos_emb: LearnedPositionalEmbeddings,\n",
    "                 classification: ClassificationHead):\n",
    "        super().__init__()\n",
    "        # Patch embeddings\n",
    "        self.patch_emb = patch_emb\n",
    "        self.pos_emb = pos_emb\n",
    "        # Classification head\n",
    "        self.classification = classification\n",
    "        # Make copies of the transformer layer\n",
    "        self.transformer_layers = clone_module_list(transformer_layer, n_layers)\n",
    "\n",
    "        # `[CLS]` token embedding\n",
    "        self.cls_token_emb = nn.Parameter(torch.randn(1, 1, transformer_layer.size), requires_grad=True)\n",
    "        # Final normalization layer\n",
    "        self.ln = nn.LayerNorm([transformer_layer.size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        # Get patch embeddings. This gives a tensor of shape `[patches, batch_size, d_model]`\n",
    "        x = self.patch_emb(x)\n",
    "        # Concatenate the `[CLS]` token embeddings before feeding the transformer\n",
    "        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)\n",
    "        x = torch.cat([cls_token_emb, x])\n",
    "        # Add positional embeddings\n",
    "        x = self.pos_emb(x)\n",
    "\n",
    "        # Pass through transformer layers with no attention masking\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x=x, mask=None)\n",
    "\n",
    "        # Get the transformer output of the `[CLS]` token (which is the first in the sequence).\n",
    "        x = x[0]\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Classification head, to get logits\n",
    "        x = self.classification(x)\n",
    "\n",
    "        #\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
