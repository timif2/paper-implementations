{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Attention Networks (GAT) in PyTorch\n",
    "\n",
    "This code is a PyTorch adaptation of the Graph Attention Networks (GAT) paper. This uses the labml package, and is primarily based off of code from labml.\n",
    "\n",
    "In essence, GATs are designed to operate on graph-structured data, where a graph comprises nodes connected by edges. For instance, in the Cora dataset, nodes represent research papers, and edges represent citations between the papers.\n",
    "\n",
    "GAT employs masked self-attention, which is akin to the mechanism used in transformers. It consists of stacked graph attention layers, where each layer receives node embeddings as input and produces transformed embeddings as output. These layers allow nodes to attend to the embeddings of other connected nodes, facilitating the learning of relationships within the graph. The implementation includes detailed descriptions of the graph attention layer operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from labml_helpers.module import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph attention layer\n",
    "\n",
    "This is a single graph attention layer.\n",
    "A GAT is made up of multiple such layers.\n",
    "\n",
    "It takes\n",
    "$$\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$$,\n",
    "where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input\n",
    "and outputs\n",
    "$$\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$$,\n",
    "where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* `in_features`, $F$, is the number of input features per node\n",
    "* `out_features`, $F'$, is the number of output features per node\n",
    "* `n_heads`, $K$, is the number of attention heads\n",
    "* `is_concat` whether the multi-head results should be concatenated or averaged\n",
    "* `dropout` is the dropout probability\n",
    "* `leaky_relu_negative_slope` is the negative slope for leaky relu activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(Module):\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, n_heads: int,\n",
    "                 is_concat: bool = True,\n",
    "                 dropout: float = 0.6,\n",
    "                 leaky_relu_negative_slope: float = 0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_concat = is_concat\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Calculate the number of dimensions per head\n",
    "        if is_concat:\n",
    "            assert out_features % n_heads == 0\n",
    "            # If we are concatenating the multiple heads\n",
    "            self.n_hidden = out_features // n_heads\n",
    "        else:\n",
    "            # If we are averaging the multiple heads\n",
    "            self.n_hidden = out_features\n",
    "\n",
    "        # Linear layer for initial transformation;\n",
    "        # i.e. to transform the node embeddings before self-attention\n",
    "        self.linear = nn.Linear(in_features, self.n_hidden * n_heads, bias=False)\n",
    "        # Linear layer to compute attention score $e_{ij}$\n",
    "        self.attn = nn.Linear(self.n_hidden * 2, 1, bias=False)\n",
    "        # The activation for attention score $e_{ij}$\n",
    "        self.activation = nn.LeakyReLU(negative_slope=leaky_relu_negative_slope)\n",
    "        # Softmax to compute attention $\\alpha_{ij}$\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        # Dropout layer to be applied for attention\n",
    "        self.dropout = nn.Dropout(dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* `h`, $\\mathbf{h}$ is the input node embeddings of shape `[n_nodes, in_features]`.\n",
    "* `adj_mat` is the adjacency matrix of shape `[n_nodes, n_nodes, n_heads]`.\n",
    "We use shape `[n_nodes, n_nodes, 1]` since the adjacency is the same for each head.\n",
    "\n",
    "Adjacency matrix represent the edges (or connections) among nodes.\n",
    "`adj_mat[i][j]` is `True` if there is an edge from node `i` to node `j`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n",
    "\n",
    "        # Number of nodes\n",
    "        n_nodes = h.shape[0]\n",
    "        # The initial transformation,\n",
    "        # $$\\overrightarrow{g^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$$\n",
    "        # for each head.\n",
    "        # We do single linear transformation and then split it up for each head.\n",
    "        g = self.linear(h).view(n_nodes, self.n_heads, self.n_hidden)\n",
    "        g_repeat = g.repeat(n_nodes, 1, 1)\n",
    "        g_repeat_interleave = g.repeat_interleave(n_nodes, dim=0)\n",
    "        g_concat = torch.cat([g_repeat_interleave, g_repeat], dim=-1)\n",
    "        # Reshape so that `g_concat[i, j]` is $\\overrightarrow{g_i} \\Vert \\overrightarrow{g_j}$\n",
    "        g_concat = g_concat.view(n_nodes, n_nodes, self.n_heads, 2 * self.n_hidden)\n",
    "        \n",
    "        e = self.activation(self.attn(g_concat))\n",
    "        # Remove the last dimension of size `1`\n",
    "        e = e.squeeze(-1)\n",
    "        \n",
    "        assert adj_mat.shape[0] == 1 or adj_mat.shape[0] == n_nodes\n",
    "        assert adj_mat.shape[1] == 1 or adj_mat.shape[1] == n_nodes\n",
    "        assert adj_mat.shape[2] == 1 or adj_mat.shape[2] == self.n_heads\n",
    "        # Mask $e_{ij}$ based on adjacency matrix.\n",
    "        # $e_{ij}$ is set to $- \\infty$ if there is no edge from $i$ to $j$.\n",
    "        e = e.masked_fill(adj_mat == 0, float('-inf'))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
