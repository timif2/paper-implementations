{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (ViT) implementation\n",
    "\n",
    "This project is a PyTorch implementation of the paper: An Image Is Worth 16x16 Words: Transformers For Image Recognition At Scale. It uses the labml module, and is based off of the labmlai implementation. This model can be trained on datasets, including the CIFAR-10, for instance.\n",
    "\n",
    "Vision transformers apply a pure transformer to images without any convolution layers. They split the image into patches and apply a transformer on patch embeddings. Patch embeddings are generated by applying a simple linear transformation to the flattened pixel values of the patch. After this, a standard transformer encoder is fed with the patch embeddings, along with a classification token. Then, the encoding on the token is used to classify the image with an Multi - Layer - Perceptron.\n",
    "\n",
    "When feeding the transformer with the patches, learned positional embeddings are added to the patch embeddings, because the patch embeddings do not have any information about where that patch is from. This is due to inherent properties of the Transformer architecture being unable to incorporate spatial information in this context. The positional embeddings are a set of vectors for each patch location that get trained with gradient descent along with other parameters.\n",
    "\n",
    "ViTs perform well when they are pre-trained on large datasets. The paper suggests pre-training them with a MLP classification head and then using a single linear layer when fine-tuning. Results - wise, the paper beats SOTA with a ViT pre-trained on a 300 million image dataset. They also use higher resolution images during the inference phase while keeping the patch size the same. The positional embeddings for new patch locations are calculated by interpolating learning positional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from labml_helpers.module import Module\n",
    "from labml_nn.transformers import TransformerLayer\n",
    "from labml_nn.utils import clone_module_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddings(Module):\n",
    "   \n",
    "    def __init__(self, d_model: int, patch_size: int, in_channels: int):\n",
    "     \n",
    "        super().__init__()\n",
    "\n",
    "        # We create a convolution layer with a kernel size and and stride length equal to patch size.\n",
    "        # This is equivalent to splitting the image into patches and doing a linear\n",
    "        # transformation on each patch.\n",
    "        self.conv = nn.Conv2d(in_channels, d_model, patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "      \n",
    "        # Apply convolution layer\n",
    "        x = self.conv(x)\n",
    "        # Get the shape.\n",
    "        bs, c, h, w = x.shape\n",
    "        # Rearrange to shape `[patches, batch_size, d_model]`\n",
    "        x = x.permute(2, 3, 0, 1)\n",
    "        x = x.view(h * w, bs, c)\n",
    "\n",
    "        # Return the patch embeddings\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEmbeddings(Module):\n",
    "\n",
    "    def __init__(self, d_model: int, max_len: int = 5_000):\n",
    "        super().__init__()\n",
    "        # Positional embeddings for each location\n",
    "        self.positional_encodings = nn.Parameter(torch.zeros(max_len, 1, d_model), requires_grad=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Get the positional embeddings for the given patches\n",
    "        pe = self.positional_encodings[:x.shape[0]]\n",
    "        # Add to patch embeddings and return\n",
    "        return x + pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(Module):\n",
    "\n",
    "    def __init__(self, d_model: int, n_hidden: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        # First layer\n",
    "        self.linear1 = nn.Linear(d_model, n_hidden)\n",
    "        # Activation\n",
    "        self.act = nn.ReLU()\n",
    "        # Second layer\n",
    "        self.linear2 = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # First layer and activation\n",
    "        x = self.act(self.linear1(x))\n",
    "        # Second layer\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        #\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(Module):\n",
    "\n",
    "    def __init__(self, transformer_layer: TransformerLayer, n_layers: int,\n",
    "                 patch_emb: PatchEmbeddings, pos_emb: LearnedPositionalEmbeddings,\n",
    "                 classification: ClassificationHead):\n",
    "        super().__init__()\n",
    "        # Patch embeddings\n",
    "        self.patch_emb = patch_emb\n",
    "        self.pos_emb = pos_emb\n",
    "        # Classification head\n",
    "        self.classification = classification\n",
    "        # Make copies of the transformer layer\n",
    "        self.transformer_layers = clone_module_list(transformer_layer, n_layers)\n",
    "\n",
    "        # `[CLS]` token embedding\n",
    "        self.cls_token_emb = nn.Parameter(torch.randn(1, 1, transformer_layer.size), requires_grad=True)\n",
    "        # Final normalization layer\n",
    "        self.ln = nn.LayerNorm([transformer_layer.size])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "\n",
    "        # Get patch embeddings. This gives a tensor of shape `[patches, batch_size, d_model]`\n",
    "        x = self.patch_emb(x)\n",
    "        # Concatenate the `[CLS]` token embeddings before feeding the transformer\n",
    "        cls_token_emb = self.cls_token_emb.expand(-1, x.shape[1], -1)\n",
    "        x = torch.cat([cls_token_emb, x])\n",
    "        # Add positional embeddings\n",
    "        x = self.pos_emb(x)\n",
    "\n",
    "        # Pass through transformer layers with no attention masking\n",
    "        for layer in self.transformer_layers:\n",
    "            x = layer(x=x, mask=None)\n",
    "\n",
    "        # Get the transformer output of the `[CLS]` token (which is the first in the sequence).\n",
    "        x = x[0]\n",
    "\n",
    "        # Layer normalization\n",
    "        x = self.ln(x)\n",
    "\n",
    "        # Classification head, to get logits\n",
    "        x = self.classification(x)\n",
    "\n",
    "        #\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
